{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:33:51.591588Z",
     "iopub.status.busy": "2025-04-13T15:33:51.591332Z",
     "iopub.status.idle": "2025-04-13T15:33:51.594839Z",
     "shell.execute_reply": "2025-04-13T15:33:51.594234Z",
     "shell.execute_reply.started": "2025-04-13T15:33:51.591557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Goal: predict the post's political leaning and subreddit based on title and selftext (Reddit dataset)\n",
    "\n",
    "# Problem Type: multiclass, 2-label classification\n",
    "\n",
    "# Model: Word Level LSTM with Word Embeddings\n",
    "\n",
    "# Evaluation: accuracy, precision, recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:33:51.595786Z",
     "iopub.status.busy": "2025-04-13T15:33:51.595499Z",
     "iopub.status.idle": "2025-04-13T15:33:56.024097Z",
     "shell.execute_reply": "2025-04-13T15:33:56.023213Z",
     "shell.execute_reply.started": "2025-04-13T15:33:51.595766Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# check if cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:33:56.025455Z",
     "iopub.status.busy": "2025-04-13T15:33:56.025007Z",
     "iopub.status.idle": "2025-04-13T15:33:59.694929Z",
     "shell.execute_reply": "2025-04-13T15:33:59.694186Z",
     "shell.execute_reply.started": "2025-04-13T15:33:56.025420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - Analysis: Here's a little math lesson for Ted Cruz on the Supreme Court - Lying Ted Cancun Cruz now needs a Math lesson from his kid, Ever since he cut his vacation short and came back with his tail between his legs, he's gone senile\n",
      "\n",
      "After - analysis here s a little math lesson for ted cruz on the supreme court lying ted cancun cruz now needs a math lesson from his kid ever since he cut his vacation short and came back with his tail between his legs he s gone senile\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load the data\n",
    "dataset = pd.read_csv('/kaggle/input/liberals-vs-conservatives-on-reddit-13000-posts/file_name.csv')\n",
    "text, labels = dataset[['Title', 'Text']].copy(), dataset[['Political Lean', 'Subreddit']].copy()\n",
    "\n",
    "# numerize the labels\n",
    "leaning_encoder, subreddit_encoder = LabelEncoder(), LabelEncoder()\n",
    "labels['Political Lean'], labels['Subreddit'] = leaning_encoder.fit_transform(labels['Political Lean']), subreddit_encoder.fit_transform(labels['Subreddit'])\n",
    "\n",
    "# merge titles and selftexts. if theres no selftext, title will be the sole input\n",
    "text = text.apply(lambda row: row['Title'] + ' ' + row['Text'] if pd.notna(row['Text']) else row['Title'], axis=1)\n",
    "\n",
    "# simplify the vocabulary by turning unicode to ascii\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim puntuation, remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # add a space before the punctuation to treat it seperately from the word(s) its connected to\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # keep only a-z, ! and ? in the text\n",
    "    s = re.sub(r\"[^a-z!?]+\", r\" \", s)\n",
    "    # remove trailing whitespaces\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# apply the changes\n",
    "sample_before = text[1981]\n",
    "text = text.apply(lambda row: normalizeString(row))\n",
    "sample_after = text[1981]\n",
    "\n",
    "# sample comparison\n",
    "print(f\"Before - {sample_before}\\n\\nAfter - {sample_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the word counts of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:33:59.695955Z",
     "iopub.status.busy": "2025-04-13T15:33:59.695695Z",
     "iopub.status.idle": "2025-04-13T15:34:00.058395Z",
     "shell.execute_reply": "2025-04-13T15:34:00.057564Z",
     "shell.execute_reply.started": "2025-04-13T15:33:59.695927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of word counts:\n",
      "count    12854.000000\n",
      "mean        49.827291\n",
      "std        192.413926\n",
      "min          1.000000\n",
      "25%          9.000000\n",
      "50%         13.000000\n",
      "75%         24.000000\n",
      "max       5916.000000\n",
      "dtype: float64\n",
      "\n",
      "90th, 95th and 99th percentiles of word counts:\n",
      "[83.0, 182.0, 688.3499999999967]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "count = text.apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "print(f\"Summary statistics of word counts:\\n{count.describe()}\\n\")\n",
    "print(f\"90th, 95th and 99th percentiles of word counts:\\n{[count.quantile(x) for x in (0.9, 0.95, 0.99)]}\\n\")\n",
    "\n",
    "plt.title('Distribution of the number of words')\n",
    "plt.hist(count, bins=[0, 3, 10, 20, 50, 100, 150, 200, 500, 1000], weights=[1 / len(count)] * len(count), edgecolor='black')\n",
    "plt.xticks([x for x in np.arange(0, 1001, 100)])\n",
    "plt.yticks([y for y in np.arange(0.05, 0.5, 0.05)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to set a limit on number of words: 100. The main reasons/goals are:\n",
    "--------\n",
    "1) Speed up training\n",
    "2) Prevent overfitting\n",
    "3) The starting text is the most informative. Even if a small fraction of the data (8.74%) has more than 100 words, they can still be effectively classified\n",
    "------\n",
    "It was also decided to set a minimum word count of 3 to filter unclear titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:34:00.059502Z",
     "iopub.status.busy": "2025-04-13T15:34:00.059191Z",
     "iopub.status.idle": "2025-04-13T15:34:00.128067Z",
     "shell.execute_reply": "2025-04-13T15:34:00.127381Z",
     "shell.execute_reply.started": "2025-04-13T15:34:00.059470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of sequence lengths after the cutting:\n",
      "\n",
      "count    12643.000000\n",
      "mean        25.174088\n",
      "std         28.213046\n",
      "min          3.000000\n",
      "25%          9.000000\n",
      "50%         13.000000\n",
      "75%         25.000000\n",
      "max        100.000000\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Before: sick of the west moralizing ! rant first things first no country has the right to invade another country countries don t have rights people have rights and an invasion is a fundamental violation of those rights russian soldiers should not be in ukraine killing ukrainian people that being said it s so fucking tone deaf to see what all of these people companies and countries are doing to russia because they invaded another country when the us has invaded and overthrown at least countries in the past two decades with the uk denmark poland and australia providing troops for these illegal invasions does anyone remember the dixie chicks being cancelled because they were anti war ? now suddenly american people and companies are cancelling services to russia because they re doing the same thing ? i see news anchors saying it s shocking to see war in a civilized country ! the utter hypocrisy literally makes me sick and i hope that others see the same bullshit i do rant over\n",
      "\n",
      "After: sick of the west moralizing ! rant first things first no country has the right to invade another country countries don t have rights people have rights and an invasion is a fundamental violation of those rights russian soldiers should not be in ukraine killing ukrainian people that being said it s so fucking tone deaf to see what all of these people companies and countries are doing to russia because they invaded another country when the us has invaded and overthrown at least countries in the past two decades with the uk denmark poland and australia providing troops for\n"
     ]
    }
   ],
   "source": [
    "min_count = 3\n",
    "max_count = 100\n",
    "\n",
    "text_trim, labels_trim = text[count >= min_count].reset_index(drop=True), labels[count >= min_count].reset_index(drop=True)\n",
    "\n",
    "sample_before = text_trim[78]\n",
    "text_trim = text_trim.apply(lambda x: ' '.join(x.split(' ')[:max_count]))\n",
    "sample_after = text_trim[78]\n",
    "\n",
    "print(f\"Summary statistics of sequence lengths after the cutting:\\n\\n{text_trim.apply(lambda x: len(x.split(' '))).describe()}\\n\\n\")\n",
    "\n",
    "print(f\"Before: {sample_before}\\n\\nAfter: {sample_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign each word (token) a unique index. These indices will be used for assigning each word a dense vector, which is the representation of the word that is fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:34:00.130054Z",
     "iopub.status.busy": "2025-04-13T15:34:00.129762Z",
     "iopub.status.idle": "2025-04-13T15:34:00.394118Z",
     "shell.execute_reply": "2025-04-13T15:34:00.393385Z",
     "shell.execute_reply.started": "2025-04-13T15:34:00.130033Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary of the dataset consists of 20655 tokens.\n",
      "\n",
      "Before: a cause of america s labor shortage millions with long covid\n",
      "\n",
      "After: [1, 75, 1947, 21, 475, 38, 1842, 5317, 4221, 65, 239, 240, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(df):\n",
    "    '''Form the vocabulary from the dataset. Each word will be assigned an index'''\n",
    "    # padding token is assigned to 0, start of sequence token is assigned to 1, end of sequence token is assigned to 2\n",
    "    vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2}\n",
    "    for s in df:\n",
    "        for l in s.split(' '):\n",
    "            if l not in vocab:\n",
    "                vocab[l] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "VOCABULARY = get_vocabulary(text_trim)\n",
    "print(f\"The vocabulary of the dataset consists of {len(VOCABULARY)} tokens.\\n\")\n",
    "\n",
    "text_numerized = text_trim.apply(lambda x: ([VOCABULARY['<SOS>']] + [VOCABULARY[l] for l in x.split(' ')] + [VOCABULARY['<EOS>']] +\n",
    "                                            [VOCABULARY['<PAD>']]*(max_count - len(x.split(' ')))))\n",
    "print(f\"Before: {text_trim[1972]}\\n\\nAfter: {text_numerized[1972]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe6B50d - pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:34:00.395481Z",
     "iopub.status.busy": "2025-04-13T15:34:00.395179Z",
     "iopub.status.idle": "2025-04-13T15:34:04.719104Z",
     "shell.execute_reply": "2025-04-13T15:34:04.718211Z",
     "shell.execute_reply.started": "2025-04-13T15:34:00.395450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09939481965625757\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Load the pretrained embeddings from txt into a dictionary\n",
    "def load_glove_embeddings(glove_file, vocab, embedding_dim):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in vocab:  # Only load embeddings for words in the vocabulary\n",
    "                vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float32)\n",
    "                embeddings[word] = vector\n",
    "            # Stop early if all vocab words are found\n",
    "            if len(embeddings) == len(vocab):\n",
    "                break\n",
    "    return embeddings\n",
    "\n",
    "# Initialize an embedding layer with the loaded GloVe embeddings\n",
    "def initialize_embedding_layer(vocab, glove_embeddings, embedding_dim):\n",
    "    voc_size = len(vocab)\n",
    "    embedding_matrix = torch.zeros((voc_size, embedding_dim))\n",
    "    # a counter to compute the OOV ratio (Out Of Vocabulary: how many words in our vocabulary werent covered by GloVe)\n",
    "    c = 0\n",
    "    for word, idx in vocab.items():\n",
    "        if word in glove_embeddings:\n",
    "            embedding_matrix[idx] = glove_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = torch.randn(embedding_dim)  # Random initialization for OOV words\n",
    "            c += 1\n",
    "\n",
    "    embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "    return embedding_layer, c / voc_size\n",
    "\n",
    "glove_file = '/kaggle/input/glove6b50dtxt/glove.6B.50d.txt'\n",
    "embedding_dim = 50\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_file, VOCABULARY, embedding_dim) \n",
    "glove_layer, ratio = initialize_embedding_layer(VOCABULARY, glove_embeddings, embedding_dim)\n",
    "# roughly 10% OOV rate\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data is fully processed. Now it should be wrapped into tensor datasets for feeding into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:34:04.720057Z",
     "iopub.status.busy": "2025-04-13T15:34:04.719839Z",
     "iopub.status.idle": "2025-04-13T15:34:05.031488Z",
     "shell.execute_reply": "2025-04-13T15:34:05.030559Z",
     "shell.execute_reply.started": "2025-04-13T15:34:04.720038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LibVsConDataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        '''Pass the data that will form the Dataset instance'''\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''Measure of the size of the dataset'''\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''How a data point of the dataset is retrieved'''\n",
    "        # we simply expand torch tensor indexing into the Dataset\n",
    "        text, label = self.text[idx], self.labels[idx]\n",
    "        return text, label\n",
    "\n",
    "# split the dataset into training, validation and test sets with 0.7:0.15:0.15 ratio\n",
    "text_train, text_notrain, labels_train, labels_notrain = train_test_split(text_numerized, labels_trim, test_size=0.3,\n",
    "                                                                              stratify=labels_trim, random_state=42)\n",
    "\n",
    "text_val, text_test, labels_val, labels_test = train_test_split(text_notrain, labels_notrain, test_size=0.5,\n",
    "                                                                     stratify=labels_notrain, random_state=42)\n",
    "\n",
    "# wrap the data into tensor datasets\n",
    "text_train, labels_train = torch.tensor(text_train.tolist(), dtype=torch.long), torch.tensor(labels_train.values, dtype=torch.long)\n",
    "text_val, labels_val = torch.tensor(text_val.tolist(), dtype=torch.long), torch.tensor(labels_val.values, dtype=torch.long)\n",
    "text_test, labels_test = torch.tensor(text_test.tolist(), dtype=torch.long), torch.tensor(labels_test.values, dtype=torch.long)\n",
    "\n",
    "\n",
    "# wrap the datasets into torch.utils.data.Dataset instances\n",
    "train_dataset = LibVsConDataset(text_train, labels_train)\n",
    "val_dataset = LibVsConDataset(text_val, labels_val)\n",
    "test_dataset = LibVsConDataset(text_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Its time to build the Model Architecture: LSTM with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:34:05.032629Z",
     "iopub.status.busy": "2025-04-13T15:34:05.032361Z",
     "iopub.status.idle": "2025-04-13T15:34:05.140154Z",
     "shell.execute_reply": "2025-04-13T15:34:05.139176Z",
     "shell.execute_reply.started": "2025-04-13T15:34:05.032600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class PoliticalClassifier(nn.Module):\n",
    "    '''A neural network that classifies political leaning and subreddit of a text'''\n",
    "    def __init__(self, embedding_layer, hidden_state_size, dense_size, output_size, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        # the actual hidden size will be 2 * hidden_state_size\n",
    "        self.lstm = nn.LSTM(input_size=embedding_layer.embedding_dim, hidden_size=hidden_state_size,\n",
    "                            batch_first=True, num_layers=2, bidirectional=True)\n",
    "        self.dropout_lstm = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.binary_dense = nn.Linear(in_features=hidden_state_size*2, out_features=dense_size)\n",
    "        self.relu_binary = nn.ReLU()\n",
    "        self.dropout_binary = nn.Dropout(dropout_rate)\n",
    "        self.binary_head = nn.Linear(in_features=dense_size, out_features=output_size[0])\n",
    "        self.binary_head_vector = nn.Linear(in_features=output_size[0], out_features=hidden_state_size*2)\n",
    "\n",
    "        self.multiclass_dense = nn.Linear(in_features=hidden_state_size*4,\n",
    "                                    out_features=dense_size)\n",
    "        self.relu_multi = nn.ReLU()\n",
    "        self.dropout_multi = nn.Dropout(dropout_rate)\n",
    "        self.multiclass_head = nn.Linear(in_features=dense_size,\n",
    "                                    out_features=output_size[1])\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''X is a batch'''\n",
    "        # get dense vector representations of words\n",
    "        embedded = self.embedding(X)\n",
    "        \n",
    "        # compute the actual length of each sequence in the batch (padding excluded)\n",
    "        sequence_lengths = (X != 0).sum(dim=1)\n",
    "\n",
    "        # sort the batch in decreasing order based on the sequence lengths\n",
    "        # makes the computations more efficient as the sequences with less padding are processed first\n",
    "        sorted_lengths, sorted_indices = torch.sort(sequence_lengths, descending=True)\n",
    "        embedded_sorted = embedded[sorted_indices]\n",
    "        # pass the lengths to cpu as gpu cant process them\n",
    "        sorted_lengths = sorted_lengths.cpu()\n",
    "        # pack the sorted batch\n",
    "        embedded_packed = pack_padded_sequence(embedded_sorted, sorted_lengths, batch_first=True)\n",
    "\n",
    "        # pass the packed sequence to the LSTM\n",
    "        # get the sequence of outputs, hidden state and cell state in response\n",
    "        seq_packed_output, (hidden_state, cell_state) = self.lstm(embedded_packed)\n",
    "\n",
    "        # the forward and backward hidden states at the last time step, which summarize the whole sequence\n",
    "        forward_hidden = hidden_state[-2]\n",
    "        backward_hidden = hidden_state[-1]\n",
    "        # concatenate the hidden states at last time step to get a united, final hidden state\n",
    "        # dim=1 to concatenate horizontally, along hidden state features\n",
    "        # shape - (batch_size, 2 * hidden_size)\n",
    "        final_hidden_state = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "\n",
    "        # hidden state is the summary of what the lstm learned about the sequence\n",
    "        # it is passed to decision tables (linear layers) for final decision regarding political lean and subreddit\n",
    "\n",
    "        # apply dropout first for regularization\n",
    "        hidden_regularized = self.dropout_lstm(final_hidden_state)\n",
    "        # predict political leaning\n",
    "        binary_dense_output = self.binary_dense(hidden_regularized)\n",
    "        binary_dense_activated = self.relu_binary(binary_dense_output)\n",
    "        binary_dense_dropout = self.dropout_binary(binary_dense_activated)\n",
    "        binary_logits = self.binary_head(binary_dense_dropout)\n",
    "        # map the number to a vector matching the size of lstm's hidden size\n",
    "        binary_logit_vector = self.binary_head_vector(binary_logits)\n",
    "\n",
    "        # concatenate the political leaning prediction with lstm output and pass it to multiclass head to predict subreddit\n",
    "        multiclass_input = torch.cat([binary_logit_vector, hidden_regularized], dim=1)\n",
    "        multiclass_dense_output = self.multiclass_dense(multiclass_input)\n",
    "        multiclass_dense_activated = self.relu_multi(multiclass_dense_output)\n",
    "        multiclass_dense_dropout = self.dropout_multi(multiclass_dense_activated)\n",
    "        multi_logits = self.multiclass_head(multiclass_dense_dropout)\n",
    "        \n",
    "        # pass the raw logits to the loss function\n",
    "        return binary_logits, multi_logits\n",
    "        \n",
    "# input size resembles the size of embedding vector that each word gets as its representation\n",
    "LSTM = PoliticalClassifier(embedding_layer=glove_layer, hidden_state_size=64, dense_size=64, dropout_rate=0.2, output_size=(1, len(subreddit_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Training the model to classify political leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:34:05.141784Z",
     "iopub.status.busy": "2025-04-13T15:34:05.141416Z",
     "iopub.status.idle": "2025-04-13T15:37:17.298664Z",
     "shell.execute_reply": "2025-04-13T15:37:17.297875Z",
     "shell.execute_reply.started": "2025-04-13T15:34:05.141748Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.7079457640647888\t\tTrain Accuracy: 0.6434764862060547\t\tTrain F1: 0.7742672562599182\n",
      "Val Loss: 0.6487837622563045\t\tVal Accuracy: 0.6484376788139343\t\tVal F1: 0.783832311630249\n",
      "\n",
      "Learning Rate: 0.001\n",
      "Val Loss improved. New weights saved.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6710571050643921\t\tTrain Accuracy: 0.6470615267753601\t\tTrain F1: 0.7827916741371155\n",
      "Val Loss: 0.650079049170017\t\tVal Accuracy: 0.6515624523162842\t\tVal F1: 0.7848672866821289\n",
      "\n",
      "Learning Rate: 0.001\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.5704354643821716\t\tTrain Accuracy: 0.6474126577377319\t\tTrain F1: 0.7826016545295715\n",
      "Val Loss: 0.6510117640097937\t\tVal Accuracy: 0.6500001549720764\t\tVal F1: 0.7848461866378784\n",
      "\n",
      "Learning Rate: 0.001\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.5113189220428467\t\tTrain Accuracy: 0.6474125385284424\t\tTrain F1: 0.7826383709907532\n",
      "Val Loss: 0.6518863091866174\t\tVal Accuracy: 0.6468751430511475\t\tVal F1: 0.7832949161529541\n",
      "\n",
      "Learning Rate: 0.001\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6515437364578247\t\tTrain Accuracy: 0.6471495628356934\t\tTrain F1: 0.7822904586791992\n",
      "Val Loss: 0.6496770401795703\t\tVal Accuracy: 0.6500000357627869\t\tVal F1: 0.7828143835067749\n",
      "\n",
      "Learning Rate: 0.001\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.5758161544799805\t\tTrain Accuracy: 0.6473249197006226\t\tTrain F1: 0.782340943813324\n",
      "Val Loss: 0.6494363149007161\t\tVal Accuracy: 0.6468750238418579\t\tVal F1: 0.7822950482368469\n",
      "\n",
      "Learning Rate: 0.001\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6231606602668762\t\tTrain Accuracy: 0.6472368836402893\t\tTrain F1: 0.7830853462219238\n",
      "Val Loss: 0.6503238519032796\t\tVal Accuracy: 0.6453125476837158\t\tVal F1: 0.781828761100769\n",
      "\n",
      "Learning Rate: 0.001\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6709322333335876\t\tTrain Accuracy: 0.6470614671707153\t\tTrain F1: 0.7826330661773682\n",
      "Val Loss: 0.6503831942876183\t\tVal Accuracy: 0.6453126072883606\t\tVal F1: 0.7810215353965759\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 7 epochs\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6781366467475891\t\tTrain Accuracy: 0.647061824798584\t\tTrain F1: 0.7828947901725769\n",
      "Val Loss: 0.6514433771371841\t\tVal Accuracy: 0.6437499523162842\t\tVal F1: 0.7787309288978577\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 8 epochs\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6781828999519348\t\tTrain Accuracy: 0.6470617055892944\t\tTrain F1: 0.7823022603988647\n",
      "Val Loss: 0.6474986732006073\t\tVal Accuracy: 0.6500000357627869\t\tVal F1: 0.7847413420677185\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "Val Loss improved. New weights saved.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6669698357582092\t\tTrain Accuracy: 0.6470614671707153\t\tTrain F1: 0.7825986742973328\n",
      "Val Loss: 0.6484769960244496\t\tVal Accuracy: 0.6484376788139343\t\tVal F1: 0.7832258343696594\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.7168341875076294\t\tTrain Accuracy: 0.6469734311103821\t\tTrain F1: 0.7824225425720215\n",
      "Val Loss: 0.6474761039018629\t\tVal Accuracy: 0.6500000357627869\t\tVal F1: 0.7850966453552246\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "Val Loss improved. New weights saved.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6155133843421936\t\tTrain Accuracy: 0.6472371220588684\t\tTrain F1: 0.7828144431114197\n",
      "Val Loss: 0.6501401513814926\t\tVal Accuracy: 0.6453126072883606\t\tVal F1: 0.7812687754631042\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.5790963768959045\t\tTrain Accuracy: 0.6473247408866882\t\tTrain F1: 0.7825551629066467\n",
      "Val Loss: 0.65224439005057\t\tVal Accuracy: 0.6421875953674316\t\tVal F1: 0.7783222794532776\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.693325400352478\t\tTrain Accuracy: 0.6469734907150269\t\tTrain F1: 0.782433032989502\n",
      "Val Loss: 0.6511638512214023\t\tVal Accuracy: 0.6437501907348633\t\tVal F1: 0.7795615196228027\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.571071445941925\t\tTrain Accuracy: 0.6473248600959778\t\tTrain F1: 0.7826723456382751\n",
      "Val Loss: 0.646481270591418\t\tVal Accuracy: 0.651562511920929\t\tVal F1: 0.7846489548683167\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "Val Loss improved. New weights saved.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6080301403999329\t\tTrain Accuracy: 0.6472369432449341\t\tTrain F1: 0.7825062870979309\n",
      "Val Loss: 0.6523270994424819\t\tVal Accuracy: 0.6421874761581421\t\tVal F1: 0.7776492238044739\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.59514319896698\t\tTrain Accuracy: 0.6472368240356445\t\tTrain F1: 0.7822070121765137\n",
      "Val Loss: 0.6495170940955481\t\tVal Accuracy: 0.6468750238418579\t\tVal F1: 0.7819384336471558\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.7890041470527649\t\tTrain Accuracy: 0.6467984318733215\t\tTrain F1: 0.7823588848114014\n",
      "Val Loss: 0.6493749767541885\t\tVal Accuracy: 0.6468750834465027\t\tVal F1: 0.7820980548858643\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.5941498875617981\t\tTrain Accuracy: 0.6472368240356445\t\tTrain F1: 0.7826197743415833\n",
      "Val Loss: 0.6492973874012629\t\tVal Accuracy: 0.6468750238418579\t\tVal F1: 0.7824915647506714\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6083983182907104\t\tTrain Accuracy: 0.6472371816635132\t\tTrain F1: 0.7827908396720886\n",
      "Val Loss: 0.6492559095223748\t\tVal Accuracy: 0.6468750834465027\t\tVal F1: 0.7827732563018799\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 22\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6357818245887756\t\tTrain Accuracy: 0.6471493244171143\t\tTrain F1: 0.7827799916267395\n",
      "Val Loss: 0.6475202699502306\t\tVal Accuracy: 0.6499999761581421\t\tVal F1: 0.7853052616119385\n",
      "\n",
      "Learning Rate: 0.0001\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 23\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6113739609718323\t\tTrain Accuracy: 0.6472371220588684\t\tTrain F1: 0.7824456691741943\n",
      "Val Loss: 0.6511732260386152\t\tVal Accuracy: 0.6437501311302185\t\tVal F1: 0.779566764831543\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "No improvement for 7 epochs\n",
      "\n",
      "Epoch 24\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.7078234553337097\t\tTrain Accuracy: 0.6469736099243164\t\tTrain F1: 0.7821714282035828\n",
      "Val Loss: 0.6502554029226301\t\tVal Accuracy: 0.6453126072883606\t\tVal F1: 0.7814823389053345\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "No improvement for 8 epochs\n",
      "\n",
      "Epoch 25\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6394827961921692\t\tTrain Accuracy: 0.6471490263938904\t\tTrain F1: 0.7827897667884827\n",
      "Val Loss: 0.6503440062204996\t\tVal Accuracy: 0.645312488079071\t\tVal F1: 0.7796949744224548\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "No improvement for 9 epochs\n",
      "\n",
      "Epoch 26\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.7681607604026794\t\tTrain Accuracy: 0.6467984318733215\t\tTrain F1: 0.7823001742362976\n",
      "Val Loss: 0.6483958949645361\t\tVal Accuracy: 0.6484375596046448\t\tVal F1: 0.7833529114723206\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "No improvement for 10 epochs\n",
      "\n",
      "Epoch 27\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6690360903739929\t\tTrain Accuracy: 0.6470617055892944\t\tTrain F1: 0.7828080058097839\n",
      "Val Loss: 0.6465513428052265\t\tVal Accuracy: 0.6515626907348633\t\tVal F1: 0.7857277989387512\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "No improvement for 11 epochs\n",
      "\n",
      "Epoch 28\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6256470084190369\t\tTrain Accuracy: 0.6471495032310486\t\tTrain F1: 0.7825407981872559\n",
      "Val Loss: 0.6503515124320981\t\tVal Accuracy: 0.6453126072883606\t\tVal F1: 0.7819328308105469\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "No improvement for 12 epochs\n",
      "\n",
      "Epoch 29\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.6490485072135925\t\tTrain Accuracy: 0.6471494436264038\t\tTrain F1: 0.7823190689086914\n",
      "Val Loss: 0.6493750691413878\t\tVal Accuracy: 0.6468751430511475\t\tVal F1: 0.7821562886238098\n",
      "\n",
      "Learning Rate: 1.0000000000000002e-06\n",
      "No improvement for 13 epochs\n",
      "\n",
      "Epoch 30\n",
      "\n",
      "1 Batches Done\n",
      "101 Batches Done\n",
      "201 Batches Done\n",
      "\n",
      "Train Loss: 0.698988139629364\t\tTrain Accuracy: 0.6469739079475403\t\tTrain F1: 0.7825800776481628\n",
      "Val Loss: 0.6485248049100237\t\tVal Accuracy: 0.6484376788139343\t\tVal F1: 0.7836422324180603\n",
      "\n",
      "Learning Rate: 1.0000000000000002e-06\n",
      "No improvement for 14 epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import Accuracy, F1Score\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class EarlyStopping:\n",
    "    '''Custom implementation of early stopping of training (plus checkpointing)'''\n",
    "    def __init__(self, model, patience):\n",
    "        self.best_loss = float('inf')\n",
    "        self.c = 0\n",
    "        self.model = model\n",
    "        self.patience = patience\n",
    "        \n",
    "    def step(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.c = 0\n",
    "            # save the weights to come back to them if needed\n",
    "            torch.save(self.model.state_dict(), f\"weights_{self.best_loss}\")\n",
    "            print(\"Val Loss improved. New weights saved.\\n\\n\\n\")\n",
    "            return 1\n",
    "        else:\n",
    "            self.c += 1\n",
    "            print(f\"No improvement for {self.c} epochs\\n\")\n",
    "            if self.c >= self.patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                return 0\n",
    "\n",
    "# pass the model and the dataset to the specified device\n",
    "LSTM = LSTM.to(device)\n",
    "generator = torch.Generator(device=device)\n",
    "# wrap the train and val datasets into dataloaders to make them iterable for training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=True, generator=generator)\n",
    "\n",
    "# freeze the layers required for predicting subreddit to focus on the layers that predict political leaning\n",
    "layers_to_freeze = [LSTM.binary_head_vector, LSTM.multiclass_dense, LSTM.relu_multi, LSTM.dropout_multi, LSTM.multiclass_head]\n",
    "for layer in layers_to_freeze:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# optimizer - AdamW. suitable for a small text dataset due to its regularization property\n",
    "optimizer_lower = torch.optim.AdamW(LSTM.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "# reduce learning rate by 10 times if no improvement in 5 epochs\n",
    "scheduler_lower = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_lower, mode='min', factor=0.1, patience=5)\n",
    "early_stopper_lower = EarlyStopping(model=LSTM, patience=15)\n",
    "\n",
    "# loss function - binary cross entropy for political leaning. false positives are penalized as negative class (conservatives) are underrepresented\n",
    "loss_fn_lower = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_epoch(dataloader, model, loss_fn, optimizer, task):\n",
    "    '''An epoch of training the model'''\n",
    "    # set the model into training mode\n",
    "    model.train()\n",
    "    # initialize metrics\n",
    "    accuracy = Accuracy(task='binary') if task == 'binary' else Accuracy(task='multiclass', num_classes=len(subreddit_encoder.classes_))\n",
    "    f1 = F1Score(task='binary') if task == 'binary' else F1Score(task='multiclass', num_classes=len(subreddit_encoder.classes_), average=\"macro\")\n",
    "    accuracy_score, f1_score = 0, 0\n",
    "    # pass each batch to the model\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # pass the data to the device we train on\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # make prediction\n",
    "        pred1, pred2 = model(X)\n",
    "        # take the required prediction\n",
    "        pred = pred1 if task == 'binary' else pred2\n",
    "        # take the required labels\n",
    "        labels = y[:, 0].unsqueeze(-1).float() if task == 'binary' else y[:, 1]\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Apply gradient clipping\n",
    "        # perform optimization step (update gradients)\n",
    "        optimizer.step()\n",
    "        # reset the counters\n",
    "        optimizer.zero_grad()\n",
    "        if not batch % 100:\n",
    "            print(f\"{batch+1} Batches Done\")\n",
    "\n",
    "        # transform logits to probabilities\n",
    "        pred_proba = torch.sigmoid(pred)\n",
    "        # transform probabilities to labels\n",
    "        pred_label = torch.where(pred_proba > 0.5, 1, 0)\n",
    "        accuracy_score += accuracy(pred_label, labels) / len(dataloader)\n",
    "        f1_score += f1(pred_label, labels) / len(dataloader)\n",
    "        \n",
    "    print(f\"\\nTrain Loss: {loss}\\t\\tTrain Accuracy: {accuracy_score}\\t\\tTrain F1: {f1_score}\")\n",
    "\n",
    "\n",
    "def val_epoch(dataloader, model, loss_fn, scheduler, early_stopper, task):\n",
    "    \n",
    "    # bring the model into evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    accuracy = Accuracy(task='binary') if task == 'binary' else Accuracy(task='multiclass', num_classes=len(subreddit_encoder.classes_))\n",
    "    f1 = F1Score(task='binary') if task == 'binary' else F1Score(task='multiclass', num_classes=len(subreddit_encoder.classes_), average=\"macro\")\n",
    "    accuracy_score, f1_score = 0, 0\n",
    "    # compute the loss across the whole validation set\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # make predictions\n",
    "        pred1, pred2 = model(X)\n",
    "        pred = pred1 if task == 'binary' else pred2\n",
    "        labels = y[:, 0].unsqueeze(-1).float() if task == 'binary' else y[:, 1]\n",
    "        # compute loss\n",
    "        loss = loss_fn(pred, labels)\n",
    "        val_loss += loss.item() / len(dataloader)\n",
    "        \n",
    "        # transform logits to probabilities\n",
    "        pred_proba = torch.sigmoid(pred)\n",
    "        # transform probabilities to labels\n",
    "        pred_label = torch.where(pred_proba > 0.5, 1, 0)\n",
    "        accuracy_score += accuracy(pred_label, labels) / len(dataloader)\n",
    "        f1_score += f1(pred_label, labels) / len(dataloader)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss}\\t\\tVal Accuracy: {accuracy_score}\\t\\tVal F1: {f1_score}\\n\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "    scheduler.step(val_loss)\n",
    "    return early_stopper.step(val_loss)\n",
    "\n",
    "run = True\n",
    "if run:\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n\")\n",
    "        train_epoch(train_dataloader, LSTM, loss_fn_lower, optimizer_lower, task='binary')\n",
    "        early_stopping_response = val_epoch(val_dataloader, LSTM, loss_fn_lower, scheduler_lower, early_stopper_lower, task='binary')\n",
    "        if early_stopping_response == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While the model did not achieve the desired performance in this experiment, this project provided valuable insights into applying deep learning to text classification. The limited dataset size and potential for further hyperparameter tuning are key areas to address in future iterations. I'm eager to apply the knowledge gained from this project to future endeavors."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8542,
     "sourceId": 11957,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1968964,
     "sourceId": 3804783,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
